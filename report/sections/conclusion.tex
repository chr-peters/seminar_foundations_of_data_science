\section{Conclusion}
\label{conclusion}

In this article, it was demonstrated how a weak learning algorithm can be boosted into a strong learner by applying the
AdaBoost algorithm.
The hypothesis class of decision stumps was identified to be efficiently learnable using an ERM algorithm.
Thus, the AdaBoost algorithm is also efficient when using decision stumps as the base learner.

It was also shown that AdaBoost is a strong learner for the hypothesis class $L(B, T)$, which tells us
that the true error of AdaBoost can be arbitrarily decreased by increasing the size of the training set
(if the realizable assumption holds).
Furthermore, it was demonstrated how the number of boosting rounds $T$ can lead to arbitrarily complex models and
and how it can be used to control the bias-complexity tradeoff.

All of these properties make AdaBoost a useful algorithm for practical applications of machine learning.
It is a great example, of how a purely theoretical question has led to the creation of an algorithm that became 
hugely popular and is still widely used in practical applications.
