\section{AdaBoost}
\label{sec:adaboost}

In the previous section it was shown how an efficient weak learner can be constructed by applying the ERM rule to
the class of decision stumps.
This section presents the AdaBoost (Adaptive Boosting) algorithm, a proceducre that shows 
how weak learners such as decision stumps can be
used efficiently to find a hypothesis with an arbitrarily low empirical error $L_S(h)$ on a training sequence $S$.

The goal of AdaBoost is to invoke the weak learner multiple times on the training data and then to combine
the resulting hypotheses similar to a weighted majority vote.
Let $T$ be the number of times that AdaBoost invokes the weak learner on the training data. Then the resulting
output hypothesis of AdaBoost has the following form:
\begin{linenomath*}
    $$
    h(x) = \text{sign}\left( \sum_{t=1}^T w_t h_t(x) \right)
    $$
\end{linenomath*}
Here, $h_t(x)$ is the output hypothesis of the weak learner in iteration $t$ and $w_t$ is the corresponding weight
that AdaBoost assigns to this hypothesis.

In the first iteration $t=1$, the AdaBoost algorithm assigns an equal weight $D_i^{(1)} = \frac{1}{m}$ to each example
in the training sequence $S$ and then invokes the weak learner on the weighted training sequence.
The error of the resulting hypothesis is computed according to
\begin{linenomath*}
    $$
    \epsilon_t = \sum_{i=1}^m D_i^{(t)} \mathds{1}_{\left[ h_t(\mathbf{x}_i) \neq y_i \right]}
    $$
\end{linenomath*}
and is at most $\frac{1}{2}-\gamma$.

The weight $w_t$ that is assigned to a hypothesis in boosting round $t$ is computed as follows:
\begin{linenomath*}
    $$
    w_t = \frac{1}{2} \text{log} \left( \frac{1}{\epsilon_t} - 1 \right)
    $$
\end{linenomath*}
As we can see, the smaller the error $\epsilon_t$ of the hypothesis $h_t$ is, the bigger the weight $w_t$ will be.

At the end of each iteration, the weights $D_i^{(t)}$ of each training example are updated according to
\begin{linenomath*}
    $$
    D_i^{(t+1)} = \frac{D_i^{(t)} \text{exp} \left( -w_t y_i h_t(\mathbf{x}_i) \right)}{
        \sum_{j=1}^m D_j^{(t)} \text{exp} \left( -w_t y_j h_t(\mathbf{x}_j) \right) }
    $$
\end{linenomath*}
This update assigns a higher weight to those examples, that weren't correctly classified by $h_t$.

In short, the Adaboost algorithm performs the following steps in each of the $T$ iterations:
\begin{enumerate}
    \item Invoke the weak learner on the training sequence $S$ weighted by $\mathbf{D}^{(t)}$
    \item Assign a weight $w_t$ to the output hypothesis $h_t$ of the weak learner. Hypotheses with a smaller
        training error $\epsilon_t$ will get a higher weight.
    \item Compute a new weight vector $\mathbf{D}^{(t+1)}$ that gives a higher weight to incorrectly classified
        examples.
\end{enumerate}

The computational complexity of AdaBoost essentially consists of invoking the weak learning algorithm $T$ times on
the training data. Thus, if the weak learner can be implemented efficiently (as it is the case for decision stumps),
AdaBoost is also efficient.

On top of that, it can be shown~\cite{SSBD14}, 
that the training error $L_S(h)$ of the AdaBoost hypothesis $h$ decreases exponentially
in the number of boosting rounds $T$:
\begin{linenomath*}
    $$
    L_S(h) = \frac{1}{m} \sum_{i=1}^m \mathds{1}_{\left[ h(\mathbf{x}_i) \neq y_i \right]} \leq e^{-2 \gamma^2 T}
    $$
\end{linenomath*}
Here, $\gamma$ describes the $\gamma$-weak learner as defined in Definition~\ref{def:weak-learner}.
This means, that AdaBoost will achieve an arbitrarily low training error and is still an efficient algorithm.

In practical applications however, it is more important to also achieve a good out of sample error. The next section will
show that the true risk $L_\mathcal{D}(h)$ of the AdaBoost hypothesis will also be small by taking a look at the
hypothesis class that resembles all the possible output hypotheses of AdaBoost.

\subsection{AdaBoost Out-of-Sample Performance}

The hypothesis class of AdaBoost is parameterized by the hypothesis class $B$ of the weak learner it uses as well as by
the number of boosting rounds $T$. Formally, it is given as follows:
\begin{linenomath*}
    $$
    L(B, T) = \left \{ x \mapsto \text{sign} \left( \sum_{t=1}^T w_t h_t(x) \right): \ 
        w \in \mathbb{R}^T, \  h_t \in B \right \}
    $$
\end{linenomath*}

The fundamental theorem of statistical learning~\cite[chapter 6]{SSBD14} states that a hypothesis class is PAC-learnable
if its VC dimension is finite.
This means that if the VC dimension of $L(B, T)$ is finite, there exists a threshold 
$m_\mathcal{H}(\epsilon, \delta) \in \mathbb{N}$ for every $\epsilon, \delta \in (0, 1)$, 
such that $L_\mathcal{D}(L(B, T)) \leq \epsilon$ 
(with confidence $1-\delta$)
for every distribution $\mathcal{D}$ when training AdaBoost on at least $m_\mathcal{H}(\epsilon, \delta)$ training
examples. This tells us that even the Out-of-Sample error of AdaBoost can be reduced arbitrarily (if the realizable
assumption holds), by increasing the amount of training examples.

It can be shown that an upper bound of the VC dimension of $L(B, T)$ is linear in the VC dimension of the hypothesis
class $B$ of the weak learner and also linear in $T$.
This means that if the VC dimension of $B$ is finite, so is the VC dimension of $L(B, T)$.

In the case of $B$ being the hypothesis class of all decision stumps, the VC dimension of $B$ is 2,
so it is clearly finite.
It follows from the fundamental theorem of statistical learning, that when using decision stumps as the hypothesis
class for the weak learner, AdaBoost is a PAC learner for the class $L(B, T)$.
Furthermore, if the weak learner for $B$ can be implemented efficiently (as we have shown for decision stumps), AdaBoost
is also efficient.
