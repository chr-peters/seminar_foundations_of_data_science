\section{Weak Learnability}

Assuming that the realizable assumption~\cite[chapter 2]{SSBD14} holds, the generalization error
$L_{(\mathcal{D}, f)}(h)$ of a PAC learner with respect to a distribution $\mathcal{D}$ and a
labeling function $f$ can by the definition of PAC learning~\mbox{\cite[chapter 2]{SSBD14}} be reduced to an arbitrarily
small number (with confidence of $1-\delta$) by increasing the sample size of the training sequence $S$.
This, however, can be computationally infeasible in practical applications.

The concept of weak learnability aims to relax the requirement that the generalization error of a hypothesis must
become arbitrarily small the more the sample size is increased.
For weak learning, it is sufficient that the learning algorithm yields a hypothesis $h$, that performs only slightly
better than a random guess.
One can think of weak learning as applying a simple rule which isn't fully capable of modeling the data generating process,
but can still learn a little bit about the underlying problem so that it performs better than random.

Formally, the definition of a weak learner differs only slightly from the definition of PAC learning.
In the situation of a two-class classification problem, the definition of a weak learner, can be given as follows:

\begin{definition}{($\gamma$-weak-learner)}
\label{def:weak-learner}
An Algorithm $A$ is a $\gamma$-weak-learner for a hypothesis class $\mathcal{H}$ if for every $\delta \in (0, 1)$ there
exists a threshold $m_\mathcal{H}(\delta) \in \mathbb{N}$
such that for every distribution $\mathcal{D}$ over the instance space $\mathcal{X}$
and for every labeling function $f: \mathcal{X} \rightarrow \{\pm 1\}$ if running $A$ on $m \geq m_\mathcal{H}$ training
examples, it will yield a hypothesis $h$ such that with probability of at least $1-\delta$ the generalization error
$L_{(\mathcal{D}, f)}(h)$ is at most $\frac{1}{2} - \gamma$, provided that the realizable assumption holds.
\end{definition}

The parameter $\gamma$ in Definition \ref{def:weak-learner} tells us how well we can expect the weak learner to perform.
For example if a learning algorithm is a $1\%$-weak-learner for a class $\mathcal{H}$, then the generalization
error can at most be $49\%$ provided that first, the learner didn't fail
(which can happen with probability $\delta$ of drawing a bad sample from $\mathcal{D}$)
and second, the learner was run on at least $m_\mathcal{H}(\delta)$ training examples.

It still remains the question of how to obtain a weak learning algorithm for a class $\mathcal{H}$.
We already saw that applying the ERM rule to complex hypothesis classes can be computationally hard.
But what if we choose a simple hypothesis class $B$ instead, where the ERM rule can be applied efficiently?
It follows directly from Definition~\ref{def:weak-learner} that this can only work if the new algorithm
$\text{ERM}_B$ has an error of at most $\frac{1}{2} - \gamma$ for every sample that was labeled by a hypothesis from
$\mathcal{H}$. In this case, applying the ERM rule with respect to the simpler class $B$ would yield a weak learner
for $\mathcal{H}$.

\subsection{An Efficient ERM Algorithm for Decision Stumps}

One such hypothesis class, where the ERM algorithm can be implemented efficiently, is the class of decision stumps.
On the instance space $\mathcal{X} = \mathbb{R}^d$,  this class is given as
\begin{equation*}
    \mathcal{H}_{DS} = \{ \mathbf{x} \mapsto \text{sign}\left( \theta - x_i \right) \cdot b: \ 
        \theta \in \mathbb{R}, i \in \left[ d \right], b \in \{ \pm 1 \} \}
\end{equation*}
